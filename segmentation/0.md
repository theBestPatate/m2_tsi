# Linear regression ...

::: {.callout-note}
This lazy summary assumes familiarity with linear regressions and is not a learning material.
Why is there a linear regression section in the segmentation course ? Good question !
:::
## Standard equation 
$$
Y = X\theta + \epsilon
$$
with $\epsilon$ the model bias

## Estimating $\theta$


\begin{aligned}
Y &= X\theta + \epsilon \\[6pt]
Y - X\theta &= \epsilon \\[6pt]
(Y - X\theta)^{T}(Y - X\theta) &= \epsilon^{T}\epsilon \\[6pt]
Y^{T}Y - Y^{T}X\theta - \theta^{T}X^{T}Y + \theta^{T}X^{T}X\theta &= \epsilon^{T}\epsilon \\[6pt]
\frac{1}{N}(Y^{T}Y - Y^{T}(X\theta) - (X\theta)^{T}Y + \theta^{T}X^{T}X\theta &)= \frac{1}{N}\epsilon^{T}\epsilon = \text{Mean Squared Error (MSE)} \\[6pt]

\frac{1}{N}(Y^{T}Y - 2 Y^{T}(X\theta) + \theta^{T}X^{T}X\theta &)= \frac{1}{N}\epsilon^{T}\epsilon = \text{Mean Squared Error (MSE)}
\end{aligned}

Assuming that this relation stands:
$$
\begin{aligned}
\frac{\partial}{\partial V}\bigl(V^{T} A V\bigr) = (A + A^{T})\,V 
\end{aligned}
$$

Differentiating the first relation with respect to $\theta$ yields:

\begin{aligned}
-2\,Y^{T}X + 2\,X^{T}X\theta &= 0 \\[6pt]
\theta &= (Y^{T}X)(X^{T}X)^{-1}
\end{aligned}
