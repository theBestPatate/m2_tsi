[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to Computer Vision and Signal Processing",
    "section": "",
    "text": "Introduction\nThis is a subset of my notes for my master 2 in Computer Vision in Signal Processing with some extras.\nI will update it progressively.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "machine_learning/0.html",
    "href": "machine_learning/0.html",
    "title": "1  Linear regression …",
    "section": "",
    "text": "1.1 Standard equation\n\\[\nY = X\\theta + \\epsilon\n\\] with \\(\\epsilon\\) the model bias",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear regression ...</span>"
    ]
  },
  {
    "objectID": "machine_learning/0.html#estimating-theta",
    "href": "machine_learning/0.html#estimating-theta",
    "title": "1  Linear regression …",
    "section": "1.2 Estimating \\(\\theta\\)",
    "text": "1.2 Estimating \\(\\theta\\)\n\\[\\begin{aligned}\nY &= X\\theta + \\epsilon \\\\[6pt]\nY - X\\theta &= \\epsilon \\\\[6pt]\n(Y - X\\theta)^{T}(Y - X\\theta) &= \\epsilon^{T}\\epsilon \\\\[6pt]\nY^{T}Y - Y^{T}X\\theta - \\theta^{T}X^{T}Y + \\theta^{T}X^{T}X\\theta &= \\epsilon^{T}\\epsilon \\\\[6pt]\n\\frac{1}{N}(Y^{T}Y - Y^{T}(X\\theta) - (X\\theta)^{T}Y + \\theta^{T}X^{T}X\\theta &)= \\frac{1}{N}\\epsilon^{T}\\epsilon = \\text{Mean Squared Error (MSE)} \\\\[6pt]\n\n\\frac{1}{N}(Y^{T}Y - 2 Y^{T}(X\\theta) + \\theta^{T}X^{T}X\\theta &)= \\frac{1}{N}\\epsilon^{T}\\epsilon = \\text{Mean Squared Error (MSE)}\n\\end{aligned}\\]\nAssuming that this relation stands: \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial V}\\bigl(V^{T} A V\\bigr) = (A + A^{T})\\,V\n\\end{aligned}\n\\]\nDifferentiating the first relation with respect to \\(\\theta\\) yields:\n\\[\\begin{aligned}\n-2\\,Y^{T}X + 2\\,X^{T}X\\theta &= 0 \\\\[6pt]\n\\theta &= (Y^{T}X)(X^{T}X)^{-1}\n\\end{aligned}\\]",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear regression ...</span>"
    ]
  },
  {
    "objectID": "machine_learning/1.html",
    "href": "machine_learning/1.html",
    "title": "2  Binary Cross Entropy",
    "section": "",
    "text": "2.1 If you’re in a hurry :\nThe binary cross-entropy is the loss function that we use most of the time when performing binary classification.\nThe binary cross-entropy (for a single sample): \\[\n\\ell(y,\\hat{y}) = -\\bigl[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\bigr]\n\\]\nFor a dataset of N samples:\n\\[\n\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\bigl[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\bigr]\n\\]",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Cross Entropy</span>"
    ]
  },
  {
    "objectID": "machine_learning/1.html#where-the-f-does-it-comes-from",
    "href": "machine_learning/1.html#where-the-f-does-it-comes-from",
    "title": "2  Binary Cross Entropy",
    "section": "2.2 Where the f does it comes from ?",
    "text": "2.2 Where the f does it comes from ?\nIf you know basic statistical inference, this follows directly from the log-likelihood of the binomial distribution.\n\\[\n\\begin{aligned}\n&\\text{Let }Y_i\\sim\\operatorname{Bernoulli}(p),\\quad Y_i\\in\\{0,1\\},\\; i=1,\\dots,n,\\\\[4pt]\n&\\text{so }P(Y_i=y_i\\mid p)=p^{y_i}(1-p)^{1-y_i}.\\\\[6pt]\n&\\text{For independent samples }y=(y_1,\\dots,y_n),\\text{ the likelihood is}\\\\[4pt]\n&\\quad L(p\\mid y)=P(Y=y\\mid p)=\\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}\n= p^{\\sum_{i=1}^n y_i}\\,(1-p)^{\\,n-\\sum_{i=1}^n y_i}.\\\\[6pt]\n&\\text{The log-likelihood is}\\\\[4pt]\n&\\quad \\ell(p\\mid y)=\\log L(p\\mid y)\n= \\Big(\\sum_{i=1}^n y_i\\Big)\\log p + \\Big(n-\\sum_{i=1}^n y_i\\Big)\\log(1-p).\\\\[6pt]\n&\\text{Taking the negative log-likelihood (per-sample average) gives the binary cross-entropy:}\\\\[4pt]\n&\\quad -\\frac{1}{n}\\,\\ell(p\\mid y)\n= -\\frac{1}{n}\\sum_{i=1}^n\\bigl[y_i\\log p + (1-y_i)\\log(1-p)\\bigr].\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nI will probably add more details from an information theory perspective and graphs later on.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Cross Entropy</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html",
    "href": "segmentation/0.html",
    "title": "4  Filters",
    "section": "",
    "text": "4.1 Convolution filter\nLet \\(I_s\\) denote the starting (input) image, \\(I_e\\) the ending (output) image, and \\(C_K\\) the convolution kernel (filter) of size (2n+1)×(2n+1).\nThe discrete convolution relation at pixel (i,j) is:\n\\[\nI_e(i,j) \\;=\\; \\sum_{l=-n}^{n}\\sum_{k=-n}^{n} C_K(k,l)\\,I_s(i+k,j+l).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#convolution-filter",
    "href": "segmentation/0.html#convolution-filter",
    "title": "4  Filters",
    "section": "",
    "text": "\\(I_s(i,j)\\): intensity/value of the input image at coordinates (i,j).\n\n\\(I_e(i,j)\\): intensity/value of the output image at coordinates (i,j) after applying the convolution.\n\n\\(C_K(k,l)\\): kernel weight at offset (k,l), where k indexes columns (horizontal offset) and l indexes rows (vertical offset).\n\nThe sums run over the kernel support from -n to n in both dimensions, so the kernel size is (2n+1)×(2n+1).\n\nBoundary handling (not shown) must be specified separately (e.g., zero-padding, symmetric extension, circular convolution).",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#identity-kernel",
    "href": "segmentation/0.html#identity-kernel",
    "title": "4  Filters",
    "section": "4.2 Identity kernel",
    "text": "4.2 Identity kernel\n\n4.2.1 Formally\nLet \\(I_s\\) be the input image, \\(I_e\\) the output image, and \\(I_k\\) the identity convolution kernel. The identity kernel is the kernel that leaves the image unchanged under convolution:\n\\[\nI_e \\;=\\; I_s \\ast I_k \\;=\\; I_s.\n\\]\nIn coordinates, for all pixels (i,j):\n\\[\nI_s(i,j) =\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} I_k(k,l)\\,I_e(i+k,j+l) \\;=\\; I_e(i,j).\n\\] A canonical choice for \\(I_k\\) on a (2n+1)×(2n+1) support is the discrete delta (Kronecker) kernel:\n\\[\nI_k(k,l) \\;=\\; \\delta_{k,0}\\,\\delta_{l,0} \\;=\\;\n\\] \\[\n\\begin{cases}\n1 & (k,l)=(0,0),\\\\[4pt]\n0 & \\text{otherwise},\n\\end{cases}\n\\]\nwhich yields the pointwise identity: \\[\nI_e(i,j)=\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} \\delta_{k,0}\\delta_{l,0}\\,I_s(i+k,j+l)=I_s(i,j).\n\\]\n\nThe kernel support can be larger than a single pixel; values outside the central 1 are zero.\n\n\n\n4.2.2 Visually\nWith a 3x3 kernel:\n\n\n\nk\\l\n-1\n0\n1\n\n\n\n\n-1\n0\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n0\n0\n\n\n\nSee how it’s different from the typical Identity matrix in linear algebra\n\n\n\nk\\l\n-1\n0\n1\n\n\n\n\n-1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n0\n1",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#average-mean-filter",
    "href": "segmentation/0.html#average-mean-filter",
    "title": "4  Filters",
    "section": "4.3 Average (mean) filter",
    "text": "4.3 Average (mean) filter\nThe average (mean) filter is a linear, shift-invariant smoothing filter that replaces each pixel by the arithmetic mean of the pixels in a local neighborhood. It reduces high-frequency noise and small-scale detail while preserving general intensity trends.\nFor a square window of size (2n+1)×(2n+1) the average kernel A has constant coefficients:\n\\[\nA(k,l)=\\frac{1}{(2n+1)^2},\\qquad k,l\\in\\{-n,\\dots,n\\}.\n\\]\nConvolution with the average kernel gives the output image \\(I_e\\) from input \\(I_s\\):\n\\[\nI_e(i,j)=\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} A(k,l)\\,I_s(i+k,j+l)\n= \\frac{1}{(2n+1)^2}\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} I_s(i+k,j+l).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#median-filter",
    "href": "segmentation/0.html#median-filter",
    "title": "4  Filters",
    "section": "4.4 Median filter",
    "text": "4.4 Median filter\nThe median filter is a nonlinear, shift-invariant denoising filter that replaces each pixel with the median of the intensities in a local neighborhood.\nGiven an input image \\(I_s\\) and a local window of size (2n+1)x(2n+1), let \\(S(i,j)\\) be the multiset of pixel values in that window centered at (i,j): \\[\nS(i,j)=\\{\\,I_s(i+k,j+l)\\;:\\;k,l\\in\\{-n,\\dots,n\\}\\,\\}.\n\\] The output pixel is the median of S(i,j): \\[\nI_e(i,j)=\\operatorname{median}\\big(S(i,j)\\big).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#nagao-filter",
    "href": "segmentation/0.html#nagao-filter",
    "title": "4  Filters",
    "section": "4.5 Nagao filter",
    "text": "4.5 Nagao filter\n\n\n\nNagao filter kernel\n\n\nThe Nagao filter is an edge-preserving, local adaptive smoothing filter that chooses the output value from the neighborhood whose internal variance (or squared error) is minimal. It was introduced for noise reduction while preserving edges and fine details by selecting the most homogeneous local region among several oriented and shaped subwindows.",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/1.html",
    "href": "segmentation/1.html",
    "title": "5  Contour and edges",
    "section": "",
    "text": "5.1 Gradient filter\nHorizontal and vertical gradient filters approximate first-order spatial derivatives to detect local intensity change (edges). Let I be the input image. The discrete gradients at pixel (i,j) are often computed with simple finite-difference kernels.\nDefinition (central differences): \\[\nG_x(i,j)=\\frac{1}{2}\\big(I(i+1,j)-I(i-1,j)\\big),\n\\qquad\nG_y(i,j)=\\frac{1}{2}\\big(I(i,j+1)-I(i,j-1)\\big).\n\\] Equivalently, via convolution with 1D kernels: \\[\nG_x = I \\ast \\begin{bmatrix} -\\tfrac{1}{2} & 0 & \\tfrac{1}{2} \\end{bmatrix},\\qquad\nG_y = I \\ast \\begin{bmatrix} -\\tfrac{1}{2} \\\\[4pt] 0 \\\\[4pt] \\tfrac{1}{2} \\end{bmatrix}.\n\\]\nCommon alternative (forward/backward differences): \\[\nG_x^{+}(i,j)=I(i+1,j)-I(i,j),\\quad G_x^{-}(i,j)=I(i,j)-I(i-1,j),\n\\] and similarly for \\(G_y\\).\n(The forward differences is the prewitt operator. If you give more importance to the value in the center,you get the Sobel operator )\nGradient magnitude and orientation: \\[\nM(i,j)=\\sqrt{G_x(i,j)^2+G_y(i,j)^2},\\qquad\n\\theta(i,j)=\\operatorname{arctan}\\big(G_y(i,j),G_x(i,j)\\big).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Contour and edges</span>"
    ]
  }
]