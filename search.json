[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "An introduction to Computer Vision and Signal Processing",
    "section": "",
    "text": "Introduction\nThis is a subset of my notes for my master 2 in Computer Vision in Signal Processing with some extras.\nI will update it progressively.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "machine_learning/0.html",
    "href": "machine_learning/0.html",
    "title": "1  Linear regression …",
    "section": "",
    "text": "1.1 Standard equation\n\\[\nY = X\\theta + \\epsilon\n\\] with \\(\\epsilon\\) the model bias",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear regression ...</span>"
    ]
  },
  {
    "objectID": "machine_learning/0.html#estimating-theta",
    "href": "machine_learning/0.html#estimating-theta",
    "title": "1  Linear regression …",
    "section": "1.2 Estimating \\(\\theta\\)",
    "text": "1.2 Estimating \\(\\theta\\)\n\\[\\begin{aligned}\nY &= X\\theta + \\epsilon \\\\[6pt]\nY - X\\theta &= \\epsilon \\\\[6pt]\n(Y - X\\theta)^{T}(Y - X\\theta) &= \\epsilon^{T}\\epsilon \\\\[6pt]\nY^{T}Y - Y^{T}X\\theta - \\theta^{T}X^{T}Y + \\theta^{T}X^{T}X\\theta &= \\epsilon^{T}\\epsilon \\\\[6pt]\n\\frac{1}{N}(Y^{T}Y - Y^{T}(X\\theta) - (X\\theta)^{T}Y + \\theta^{T}X^{T}X\\theta &)= \\frac{1}{N}\\epsilon^{T}\\epsilon = \\text{Mean Squared Error (MSE)} \\\\[6pt]\n\n\\frac{1}{N}(Y^{T}Y - 2 Y^{T}(X\\theta) + \\theta^{T}X^{T}X\\theta &)= \\frac{1}{N}\\epsilon^{T}\\epsilon = \\text{Mean Squared Error (MSE)}\n\\end{aligned}\\]\nAssuming that this relation stands: \\[\n\\begin{aligned}\n\\frac{\\partial}{\\partial V}\\bigl(V^{T} A V\\bigr) = (A + A^{T})\\,V\n\\end{aligned}\n\\]\nDifferentiating the first relation with respect to \\(\\theta\\) yields:\n\\[\\begin{aligned}\n-2\\,Y^{T}X + 2\\,X^{T}X\\theta &= 0 \\\\[6pt]\n\\theta &= (Y^{T}X)(X^{T}X)^{-1}\n\\end{aligned}\\]",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Linear regression ...</span>"
    ]
  },
  {
    "objectID": "machine_learning/1.html",
    "href": "machine_learning/1.html",
    "title": "2  Binary Cross Entropy",
    "section": "",
    "text": "2.1 If you’re in a hurry :\nThe binary cross-entropy is the loss function that we use most of the time when performing binary classification.\nThe binary cross-entropy (for a single sample): \\[\n\\ell(y,\\hat{y}) = -\\bigl[y\\log(\\hat{y}) + (1-y)\\log(1-\\hat{y})\\bigr]\n\\]\nFor a dataset of N samples:\n\\[\n\\mathcal{L} = -\\frac{1}{N}\\sum_{i=1}^{N}\\bigl[y_i\\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\bigr]\n\\]",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Cross Entropy</span>"
    ]
  },
  {
    "objectID": "machine_learning/1.html#where-the-f-does-it-comes-from",
    "href": "machine_learning/1.html#where-the-f-does-it-comes-from",
    "title": "2  Binary Cross Entropy",
    "section": "2.2 Where the f does it comes from ?",
    "text": "2.2 Where the f does it comes from ?\nIf you know basic statistical inference, this follows directly from the log-likelihood of the binomial distribution.\n\\[\n\\begin{aligned}\n&\\text{Let }Y_i\\sim\\operatorname{Bernoulli}(p),\\quad Y_i\\in\\{0,1\\},\\; i=1,\\dots,n,\\\\[4pt]\n&\\text{so }P(Y_i=y_i\\mid p)=p^{y_i}(1-p)^{1-y_i}.\\\\[6pt]\n&\\text{For independent samples }y=(y_1,\\dots,y_n),\\text{ the likelihood is}\\\\[4pt]\n&\\quad L(p\\mid y)=P(Y=y\\mid p)=\\prod_{i=1}^n p^{y_i}(1-p)^{1-y_i}\n= p^{\\sum_{i=1}^n y_i}\\,(1-p)^{\\,n-\\sum_{i=1}^n y_i}.\\\\[6pt]\n&\\text{The log-likelihood is}\\\\[4pt]\n&\\quad \\ell(p\\mid y)=\\log L(p\\mid y)\n= \\Big(\\sum_{i=1}^n y_i\\Big)\\log p + \\Big(n-\\sum_{i=1}^n y_i\\Big)\\log(1-p).\\\\[6pt]\n&\\text{Taking the negative log-likelihood (per-sample average) gives the binary cross-entropy:}\\\\[4pt]\n&\\quad -\\frac{1}{n}\\,\\ell(p\\mid y)\n= -\\frac{1}{n}\\sum_{i=1}^n\\bigl[y_i\\log p + (1-y_i)\\log(1-p)\\bigr].\n\\end{aligned}\n\\]\n\n\n\n\n\n\nNote\n\n\n\nI will probably add more details from an information theory perspective and graphs later on.",
    "crumbs": [
      "Machine learning",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Binary Cross Entropy</span>"
    ]
  },
  {
    "objectID": "stochastic_model/0.html",
    "href": "stochastic_model/0.html",
    "title": "3  Filtering",
    "section": "",
    "text": "3.1 Type of filters\nWe observe a time-varying quantity x(t), but measurements are imperfect — what we actually record is y(t), which equals the true signal plus an error term:\n\\[y(t)=x(t)+b(t).\\]\nRecovering x(t) from the noisy observation y(t) is called filtering — i.e., designing a method that suppresses the noise b(t) while retaining the desired signal x(t).\nUsually we think of filters in two main groups: - Linear filters - Non-linear filters",
    "crumbs": [
      "Stochastic Model",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Filtering</span>"
    ]
  },
  {
    "objectID": "stochastic_model/0.html#type-of-filters",
    "href": "stochastic_model/0.html#type-of-filters",
    "title": "3  Filtering",
    "section": "",
    "text": "3.1.1 Why are we interested in linear filters ?\nIf the system is linear and time‑invariant, its action on any input is completely characterized by its impulse response h(t) (or equivalently its transfer function H(s)/H(ω)). Convolving the measured signal with h gives the system’s output or an estimate of the underlying signal:\n\\[\\tilde{x}(t)=y(t)\\ast h(t).\\]\nUsing linearity and y(t)=x(t)+b(t):\n\\[\\tilde{x}(t)=(x(t)+b(t))\\ast h(t)=x(t)\\ast h(t)+b(t)\\ast h(t).\\]\nIdeally, \\(b(t)\\ast h(t) = 0\\)\n\n\n3.1.2 Link with the fourier transform\nThe fourier transform turns the convolution into a simple product. \\[ TF(y(t)*h(t)) = Y(f) H(f)\\]",
    "crumbs": [
      "Stochastic Model",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Filtering</span>"
    ]
  },
  {
    "objectID": "stochastic_model/0.html#energy-and-the-energy-spectral-density",
    "href": "stochastic_model/0.html#energy-and-the-energy-spectral-density",
    "title": "3  Filtering",
    "section": "3.2 Energy and the energy spectral density",
    "text": "3.2 Energy and the energy spectral density\n\nFor a finite‑energy signal x(t) the total energy is defined as \\[\nE=\\int_{-\\infty}^{\\infty}|x(t)|^{2}dt.\n\\]\n\nLet \\(\\hat{x}(f)\\) be the Fourier transform of \\(x(t)\\) (frequency (f) in Hz).\n\nBy Parseval’s theorem, \\[\nE=\\int_{-\\infty}^{\\infty}|x(t)|^{2}dt=\\int_{-\\infty}^{\\infty}|\\hat{x}(f)|^{2}df.\n\\] The nonnegative function \\(|\\hat{x}(f)|^{2}\\) is called the energy spectral density (ESD) of \\((x(t)\\).\n\nFor any band \\([f_1,f_2]\\), \\[\n  \\int_{f_1}^{f_2}|\\hat{x}(f)|^{2}\\,df\n\\] equals the energy contained in that frequency band.\nThink of the total energy E as a cake. The Fourier transform splits the cake across frequencies; \\(|\\hat{x}(f)|^{2}\\) is the slice size (energy per unit frequency) at frequency f.\nThus \\(|\\hat{x}(f)|^{2}\\) is the energy contribution around frequency f,integrating it over a band tells you how much energy that band carries.\n\\[\n\\text{(ESD)} S_{xx}(f) = |\\hat{x}(f)|^2\n\\]\n\nUsing the Wiener-Khintchine theorem: the ESD can be defined as the fourier transform of the autocorrelation signal:\n\n\\[ S_{xx}(f) = \\int_{-\\infty}^{+\\infty} R_{xx}(\\tau)e^{-2 \\pi i f \\tau} d\\tau\\]\n\n\n\n\n\n\nWhy is the autocorrelation function only a function of the lag \\(\\tau\\)\n\n\n\nFor a (complex) stochastic process X(t) the general autocorrelation is \\[\nR_{XX}(t_1,t_2)=\\mathbb{E}\\big[X(t_1)\\,\\overline{X(t_2)}\\big],\n\\] which a priori depends on both times t1 and t2.\nIf X(t) is wide-sense stationary (WSS) then by definition:\n\nThe mean \\(\\mu(X(t))=E[X(t)]\\) is constant in time,\nthe autocorrelation is invariant under time shifts.\n\nTime-shift invariance means: for any real shift \\(\\Delta\\), \\[\nR_{XX}(t_1+\\Delta,t_2+\\Delta)=R_{XX}(t_1,t_2).\n\\] Fix any pair \\((t_1,t_2)\\).\nChoose \\(\\Delta = −t_2\\). Then \\[\nR_{XX}(t_1,t_2)=R_{XX}(t_1-t_2,0).\n\\]\nThus the value of \\(R_{XX}\\) at \\((t_1,t_2)\\) equals its value at \\((t_1−t_2,0)\\); it depends only on the difference \\(t_1−t_2\\), not on the absolute times.\nWriting \\(\\tau = t_1−t_2\\) gives the single-argument form \\[\nR_{XX}(\\tau)=\\mathbb{E}\\big[X(t+\\tau)\\,\\overline{X(t)}\\big],\n\\] which is independent of the particular t chosen because of the shift invariance.\nIf the process is zero-mean (centered), the autocovariance \\(K_{XX}\\) equals the autocorrelation \\(R_{XX}\\), so the same reduction to a single-argument function holds: \\[\nK_{XX}(t_1,t_2)=K_{XX}(\\tau)=R_{XX}(\\tau).\n\\] In short: WSS implies time-translation invariance of second-order statistics. That invariance forces the two-time function \\(R_{XX}(t_1,t_2)\\) to depend only on the time difference \\(\\tau = t_1−t_2\\).",
    "crumbs": [
      "Stochastic Model",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Filtering</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html",
    "href": "segmentation/0.html",
    "title": "4  Filters",
    "section": "",
    "text": "4.1 Convolution filter\nLet \\(I_s\\) denote the starting (input) image, \\(I_e\\) the ending (output) image, and \\(C_K\\) the convolution kernel (filter) of size (2n+1)×(2n+1).\nThe discrete convolution relation at pixel (i,j) is:\n\\[\nI_e(i,j) \\;=\\; \\sum_{l=-n}^{n}\\sum_{k=-n}^{n} C_K(k,l)\\,I_s(i+k,j+l).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#convolution-filter",
    "href": "segmentation/0.html#convolution-filter",
    "title": "4  Filters",
    "section": "",
    "text": "\\(I_s(i,j)\\): intensity/value of the input image at coordinates (i,j).\n\n\\(I_e(i,j)\\): intensity/value of the output image at coordinates (i,j) after applying the convolution.\n\n\\(C_K(k,l)\\): kernel weight at offset (k,l), where k indexes columns (horizontal offset) and l indexes rows (vertical offset).\n\nThe sums run over the kernel support from -n to n in both dimensions, so the kernel size is (2n+1)×(2n+1).\n\nBoundary handling (not shown) must be specified separately (e.g., zero-padding, symmetric extension, circular convolution).",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#identity-kernel",
    "href": "segmentation/0.html#identity-kernel",
    "title": "4  Filters",
    "section": "4.2 Identity kernel",
    "text": "4.2 Identity kernel\n\n4.2.1 Formally\nLet \\(I_s\\) be the input image, \\(I_e\\) the output image, and \\(I_k\\) the identity convolution kernel. The identity kernel is the kernel that leaves the image unchanged under convolution:\n\\[\nI_e \\;=\\; I_s \\ast I_k \\;=\\; I_s.\n\\]\nIn coordinates, for all pixels (i,j):\n\\[\nI_s(i,j) =\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} I_k(k,l)\\,I_e(i+k,j+l) \\;=\\; I_e(i,j).\n\\] A canonical choice for \\(I_k\\) on a (2n+1)×(2n+1) support is the discrete delta (Kronecker) kernel:\n\\[\nI_k(k,l) \\;=\\; \\delta_{k,0}\\,\\delta_{l,0} \\;=\\;\n\\] \\[\n\\begin{cases}\n1 & (k,l)=(0,0),\\\\[4pt]\n0 & \\text{otherwise},\n\\end{cases}\n\\]\nwhich yields the pointwise identity: \\[\nI_e(i,j)=\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} \\delta_{k,0}\\delta_{l,0}\\,I_s(i+k,j+l)=I_s(i,j).\n\\]\n\nThe kernel support can be larger than a single pixel; values outside the central 1 are zero.\n\n\n\n4.2.2 Visually\nWith a 3x3 kernel:\n\n\n\nk\\l\n-1\n0\n1\n\n\n\n\n-1\n0\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n0\n0\n\n\n\nSee how it’s different from the typical Identity matrix in linear algebra\n\n\n\nk\\l\n-1\n0\n1\n\n\n\n\n-1\n1\n0\n0\n\n\n0\n0\n1\n0\n\n\n1\n0\n0\n1",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#average-mean-filter",
    "href": "segmentation/0.html#average-mean-filter",
    "title": "4  Filters",
    "section": "4.3 Average (mean) filter",
    "text": "4.3 Average (mean) filter\nThe average (mean) filter is a linear, shift-invariant smoothing filter that replaces each pixel by the arithmetic mean of the pixels in a local neighborhood. It reduces high-frequency noise and small-scale detail while preserving general intensity trends.\nFor a square window of size (2n+1)×(2n+1) the average kernel A has constant coefficients:\n\\[\nA(k,l)=\\frac{1}{(2n+1)^2},\\qquad k,l\\in\\{-n,\\dots,n\\}.\n\\]\nConvolution with the average kernel gives the output image \\(I_e\\) from input \\(I_s\\):\n\\[\nI_e(i,j)=\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} A(k,l)\\,I_s(i+k,j+l)\n= \\frac{1}{(2n+1)^2}\\sum_{l=-n}^{n}\\sum_{k=-n}^{n} I_s(i+k,j+l).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#median-filter",
    "href": "segmentation/0.html#median-filter",
    "title": "4  Filters",
    "section": "4.4 Median filter",
    "text": "4.4 Median filter\nThe median filter is a nonlinear, shift-invariant denoising filter that replaces each pixel with the median of the intensities in a local neighborhood.\nGiven an input image \\(I_s\\) and a local window of size (2n+1)x(2n+1), let \\(S(i,j)\\) be the multiset of pixel values in that window centered at (i,j): \\[\nS(i,j)=\\{\\,I_s(i+k,j+l)\\;:\\;k,l\\in\\{-n,\\dots,n\\}\\,\\}.\n\\] The output pixel is the median of S(i,j): \\[\nI_e(i,j)=\\operatorname{median}\\big(S(i,j)\\big).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/0.html#nagao-filter",
    "href": "segmentation/0.html#nagao-filter",
    "title": "4  Filters",
    "section": "4.5 Nagao filter",
    "text": "4.5 Nagao filter\n\n\n\nNagao filter kernel\n\n\nThe Nagao filter is an edge-preserving, local adaptive smoothing filter that chooses the output value from the neighborhood whose internal variance (or squared error) is minimal. It was introduced for noise reduction while preserving edges and fine details by selecting the most homogeneous local region among several oriented and shaped subwindows.",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Filters</span>"
    ]
  },
  {
    "objectID": "segmentation/1.html",
    "href": "segmentation/1.html",
    "title": "5  Contour and edges",
    "section": "",
    "text": "5.1 Gradient filter\nHorizontal and vertical gradient filters approximate first-order spatial derivatives to detect local intensity change (edges). Let I be the input image. The discrete gradients at pixel (i,j) are often computed with simple finite-difference kernels.\nDefinition (central differences): \\[\nG_x(i,j)=\\frac{1}{2}\\big(I(i+1,j)-I(i-1,j)\\big),\n\\qquad\nG_y(i,j)=\\frac{1}{2}\\big(I(i,j+1)-I(i,j-1)\\big).\n\\] Equivalently, via convolution with 1D kernels: \\[\nG_x = I \\ast \\begin{bmatrix} -\\tfrac{1}{2} & 0 & \\tfrac{1}{2} \\end{bmatrix},\\qquad\nG_y = I \\ast \\begin{bmatrix} -\\tfrac{1}{2} \\\\[4pt] 0 \\\\[4pt] \\tfrac{1}{2} \\end{bmatrix}.\n\\]\nCommon alternative (forward/backward differences): \\[\nG_x^{+}(i,j)=I(i+1,j)-I(i,j),\\quad G_x^{-}(i,j)=I(i,j)-I(i-1,j),\n\\] and similarly for \\(G_y\\).\n(The forward differences is the prewitt operator. If you give more importance to the value in the center,you get the Sobel operator )\nGradient magnitude and orientation: \\[\nM(i,j)=\\sqrt{G_x(i,j)^2+G_y(i,j)^2},\\qquad\n\\theta(i,j)=\\operatorname{arctan}\\big(G_y(i,j),G_x(i,j)\\big).\n\\]",
    "crumbs": [
      "Segmentation",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Contour and edges</span>"
    ]
  }
]